{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e8588e",
   "metadata": {},
   "source": [
    "This is a combined retriever / hybrid retriever "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468f6ed",
   "metadata": {},
   "source": [
    "## Ensemble Retriever Doesn't seem to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77c1b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maruthienugula/RAG_learnings/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "#creating my own hybrid retriever\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6f6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]\n",
    "\n",
    "# Step 2: Dense Retriever (FAISS + HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ad935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sparse Retriever(BM25)\n",
    "sparse_retriever=BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3 ##top- k documents to retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6714a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Document 1:\n",
      "LangChain helps build LLM applications.\n",
      "\n",
      "ðŸ”¹ Document 2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      "ðŸ”¹ Document 3:\n",
      "Pinecone is a vector database for semantic search.\n",
      "\n",
      "ðŸ”¹ Document 4:\n",
      "Langchain has many types of retrievers.\n"
     ]
    }
   ],
   "source": [
    "# without ensemble retriever\n",
    "query = \"How can I build an application using LLMs?\"\n",
    "results = dense_retriever.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nðŸ”¹ Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa64e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, dense_retriever, sparse_retriever, w_dense=0.7, w_sparse=0.3):\n",
    "        self.dense = dense_retriever\n",
    "        self.sparse = sparse_retriever\n",
    "        self.w_dense = w_dense\n",
    "        self.w_sparse = w_sparse\n",
    "\n",
    "    def invoke(self, query: str, k: int = 5) -> List[Document]:\n",
    "        dense_docs = self.dense._get_relevant_documents(query,run_manager=CallbackManagerForRetrieverRun.get_noop_manager())\n",
    "        sparse_docs = self.sparse._get_relevant_documents(query,run_manager=CallbackManagerForRetrieverRun.get_noop_manager())\n",
    "\n",
    "        scores = defaultdict(float)\n",
    "        doc_map = {}\n",
    "\n",
    "        # Assign dense scores (inverse rank as proxy)\n",
    "        for rank, doc in enumerate(dense_docs):\n",
    "            doc_id = doc.page_content\n",
    "            scores[doc_id] += self.w_dense * (1 / (rank + 1))\n",
    "            doc_map[doc_id] = doc\n",
    "\n",
    "        # Assign sparse scores\n",
    "        for rank, doc in enumerate(sparse_docs):\n",
    "            doc_id = doc.page_content\n",
    "            scores[doc_id] += self.w_sparse * (1 / (rank + 1))\n",
    "            doc_map[doc_id] = doc\n",
    "\n",
    "        # Sort by combined score\n",
    "        ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [doc_map[doc_id] for doc_id, _ in ranked_docs[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3043445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 4 : Combine with Ensemble Retriever\n",
    "hybrid_retriever=HybridRetriever(\n",
    "    dense_retriever=dense_retriever,\n",
    "    sparse_retriever=sparse_retriever # 0.7 - dense default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4d99cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HybridRetriever at 0x127fcd010>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "344c833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Document 1:\n",
      "LangChain helps build LLM applications.\n",
      "\n",
      "ðŸ”¹ Document 2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      "ðŸ”¹ Document 3:\n",
      "Langchain has many types of retrievers.\n",
      "\n",
      "ðŸ”¹ Document 4:\n",
      "Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and get results\n",
    "query = \"How can I build an application using LLMs?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nðŸ”¹ Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d9883",
   "metadata": {},
   "source": [
    "## RAG Pipeline with Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6741971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "671cdb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x128088320>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x129654530>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the below context\n",
    "Context : {context},\n",
    "question : {input}\n",
    "\"\"\")\n",
    "\n",
    "llm = init_chat_model(model=\"groq:llama-3.3-70b-versatile\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "857094f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b33f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda,RunnablePassthrough\n",
    "\n",
    "def retrieve_docs(query):\n",
    "    return hybrid_retriever.invoke(query=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "def8e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def retrieve_docs(query):\n",
    "    return hybrid_retriever.invoke(query)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"docs\": RunnableLambda(retrieve_docs),\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(\n",
    "        lambda x: {\n",
    "            \"context\": \"\\n\\n\".join(doc.page_content for doc in x[\"docs\"]),\n",
    "            \"docs\": x[\"docs\"],\n",
    "            \"input\": x[\"input\"],\n",
    "        }\n",
    "    )\n",
    "    | RunnableLambda(\n",
    "        lambda x: {\n",
    "            \"answer\": (\n",
    "                prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            ).invoke(\n",
    "                {\"context\": x[\"context\"], \"input\": x[\"input\"]}\n",
    "            ),\n",
    "            \"context\": x[\"docs\"],\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59af7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"How can I build an app using LLMs?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50e7d254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer:\n",
      " {'answer': 'You can use LangChain to build an application using Large Language Models (LLMs). LangChain is a framework that helps developers build LLM applications, including agentic AI applications, and provides various tools and features to support the development process.', 'context': [Document(metadata={}, page_content='LangChain helps build LLM applications.'), Document(metadata={}, page_content='Langchain can be used to develop agentic ai application.'), Document(metadata={}, page_content='Langchain has many types of retrievers.'), Document(id='9c48d1af-d932-4123-a9e7-5fc748ae4881', metadata={}, page_content='Pinecone is a vector database for semantic search.')]}\n",
      "\n",
      " Source Documents:\n",
      "\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      "Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Doc 3: Langchain has many types of retrievers.\n",
      "\n",
      "Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "#  Ask a question\n",
    "query =  \"How can I build an app using LLMs?\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "#  Output\n",
    "print(\" Answer:\\n\", response)\n",
    "\n",
    "print(\"\\n Source Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea97fb1",
   "metadata": {},
   "source": [
    "Thought this would run this way but hell its deprecated and doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0ba5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is your Age ?\"\n",
    "docs = hybrid_retriever.invoke(query)\n",
    "\n",
    "context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "answer = llm.invoke(prompt.format(context = context,input=query))\n",
    "\n",
    "response = {\n",
    "    \"answer\":answer,\n",
    "    \"context\":docs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b085240",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#  Ask a question\u001b[39;00m\n\u001b[32m      2\u001b[39m query = {\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHow can I build an app using LLMs?\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#  Output\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Answer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, response[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3149\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3149\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3150\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3151\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3876\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3870\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3871\u001b[39m         futures = [\n\u001b[32m   3872\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3873\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3874\u001b[39m         ]\n\u001b[32m   3875\u001b[39m         output = {\n\u001b[32m-> \u001b[39m\u001b[32m3876\u001b[39m             key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3877\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3878\u001b[39m         }\n\u001b[32m   3879\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3859\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3853\u001b[39m child_config = patch_config(\n\u001b[32m   3854\u001b[39m     config,\n\u001b[32m   3855\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3856\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3857\u001b[39m )\n\u001b[32m   3858\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3861\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3863\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4880\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4865\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4866\u001b[39m \n\u001b[32m   4867\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4877\u001b[39m \n\u001b[32m   4878\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4879\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4880\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4881\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4886\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4887\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2058\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2054\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2056\u001b[39m         output = cast(\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2058\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2060\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2066\u001b[39m         )\n\u001b[32m   2067\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2068\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:435\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    434\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4737\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4735\u001b[39m                 output = chunk\n\u001b[32m   4736\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4737\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4738\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4739\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4740\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:435\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    434\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mretrieve_docs\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_docs\u001b[39m(query):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhybrid_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mHybridRetriever.invoke\u001b[39m\u001b[34m(self, query, k)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m) -> List[Document]:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     dense_docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCallbackManagerForRetrieverRun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_noop_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     sparse_docs = \u001b[38;5;28mself\u001b[39m.sparse._get_relevant_documents(query,run_manager=CallbackManagerForRetrieverRun.get_noop_manager())\n\u001b[32m     16\u001b[39m     scores = defaultdict(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:1040\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1038\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1042\u001b[39m     docs_and_similarities = (\n\u001b[32m   1043\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1044\u001b[39m             query, **kwargs_\n\u001b[32m   1045\u001b[39m         )\n\u001b[32m   1046\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:266\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.embedding_function, Embeddings):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:172\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \n\u001b[32m    166\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m embed_kwargs = (\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.query_encode_kwargs\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.query_encode_kwargs) > \u001b[32m0\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_kwargs\n\u001b[32m    171\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:124\u001b[39m, in \u001b[36mHuggingFaceEmbeddings._embed\u001b[39m\u001b[34m(self, texts, encode_kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text using the HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    111\u001b[39m \n\u001b[32m    112\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m texts = [\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multi_process:\n\u001b[32m    126\u001b[39m     pool = \u001b[38;5;28mself\u001b[39m._client.start_multi_process_pool()\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "#  Ask a question\n",
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "#  Output\n",
    "print(\" Answer:\\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\n Source Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85511359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_learnings (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
