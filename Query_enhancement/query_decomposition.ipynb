{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5810bf4d",
   "metadata": {},
   "source": [
    "##  Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad871ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maruthienugula/RAG_learnings/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03565964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and embed the document\n",
    "loader = TextLoader(\"langchain-crewai-dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=350, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c08b027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x130a26b70>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x130d1dbe0>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:llama-3.3-70b-versatile\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a193bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "Just decompose and give results don't reply\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee7fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50763117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. What is LangChain's approach to memory management?\\n2. How does LangChain utilize agents in its architecture?\\n3. What are the key differences between LangChain and CrewAI in terms of memory and agent usage?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposition_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d312db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is LangChain's approach to memory management?\n",
      "2. How does LangChain utilize agents in its architecture?\n",
      "3. What are the key differences between LangChain and CrewAI in terms of memory and agent usage?\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de260ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "Be concise and clear about what you are presenting.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = qa_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b581292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    subq_text = decomposition_chain.invoke({\"question\":user_query})\n",
    "    subq_questions = [q.strip(\" -1234567890.\").strip() for q in subq_text.split(\"\\n\") if q.strip()]\n",
    "    full_answers = []\n",
    "    for sub_q in subq_questions:\n",
    "        docs = retriever.invoke(sub_q)\n",
    "        answer = qa_chain.invoke({\"input\":sub_q,\"context\":docs})\n",
    "        full_answers.append(f\"Q: {sub_q}\\nA: {answer}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(full_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce0ee5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is LangChain's approach to memory management?\n",
      "A: LangChain's approach to memory management involves using memory modules, such as ConversationBufferMemory and ConversationSummaryMemory. These modules allow the LLM to maintain awareness of previous conversation turns and summarize long interactions to fit within token limits.\n",
      "\n",
      "Q: How does LangChain utilize agents in its architecture?\n",
      "A: LangChain utilizes agents in its architecture through a planner-executor model. Agents plan a sequence of tool invocations to achieve a goal, incorporating:\n",
      "\n",
      "1. Dynamic decision-making\n",
      "2. Branching logic\n",
      "3. Context-aware memory use across steps\n",
      "\n",
      "These agents use Large Language Models (LLMs) to reason about tool invocation, input provision, and output processing, enabling multi-step task execution and integration with various tools, such as web search, calculators, and custom APIs.\n",
      "\n",
      "Q: What is CrewAI's approach to memory and agent management for comparison?\n",
      "A: CrewAI's approach to agent management involves defining each agent with a purpose, goal, and set of tools. The framework ensures agents stay on task, contributing to the overall crew objective. Agents have defined roles (e.g., researcher, planner, executor) and operate semi-independently within a collaborative context, forming structured workflows. There is no explicit mention of memory management in the provided context.\n",
      "-----------------\n",
      "Here's a comparison of LangChain and CrewAI's approaches to memory and agent management:\n",
      "\n",
      "**Memory Management:**\n",
      "- LangChain: Utilizes memory modules (e.g., ConversationBufferMemory, ConversationSummaryMemory) to maintain awareness of previous conversations and summarize long interactions.\n",
      "- CrewAI: No explicit mention of memory management.\n",
      "\n",
      "**Agent Management:**\n",
      "- LangChain: Employs a planner-executor model with agents that plan tool invocations, incorporating dynamic decision-making, branching logic, and context-aware memory use.\n",
      "- CrewAI: Defines each agent with a purpose, goal, and set of tools, ensuring agents stay on task and contribute to the overall objective, with semi-independent operation within a collaborative context.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "\n",
    "print(final_answer)\n",
    "answers_last = qa_chain.invoke({\"input\":query,\"context\":final_answer})\n",
    "print(\"-----------------\")\n",
    "print(answers_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b785a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_learnings (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
