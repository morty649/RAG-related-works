{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800cd20c",
   "metadata": {},
   "source": [
    "### Building Chatbot With Multiple Tools Using Langgraph\n",
    "\n",
    "#### Aim\n",
    "Create a chatbot with tool capabilities from arxiv, wikipedia search and some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3dca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a2f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04949b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2025-02-01\\nTitle: A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges\\nAuthors: Majid Ghasemi, Amir Hossein Moosavi, Dariush Ebrahimi\\nSummary: Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.invoke(\"2411.18892\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af94fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=500)\n",
    "wikipedia=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wikipedia.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abc26e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Stephen Hawking\\nSummary: Stephen William Hawking (8 January 1942 – 14 March 2018) was an English theoretical astrophysicist, cosmologist, and author who was director of research at the Centre for Theoretical Cosmology at the University of Cambridge. Between 1979 and 2009, he was the Lucasian Professor of Mathematics at Cambridge, widely viewed as one of the most prestigious academic posts in the world.\\nHawking was born in Oxford into a family of physicians. In October 1959, at the age of 1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.invoke(\"What did Stephen Hawking found out ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b14f5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"ReAct-agent\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9614655",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.getenv(\"TAVILY_API_KEY\")\n",
    "if key is not None:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = key\n",
    "else:\n",
    "    raise ValueError(\"TAVILY_API_KEY is not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d04a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/64kxbqr16ps6hbrf1lzxq5dc0000gn/T/ipykernel_14101/3336424662.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tavily = TavilySearchResults()\n"
     ]
    }
   ],
   "source": [
    "# Tavily Search Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6dbfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'AI News Briefs BULLETIN BOARD for February 2026',\n",
       "  'url': 'https://radicaldatascience.wordpress.com/2026/02/02/ai-news-briefs-bulletin-board-for-february-2026/',\n",
       "  'content': 'Welcome to the AI News Briefs Bulletin Board, a timely new channel bringing you the latest industry insights and perspectives surrounding the field of AI including deep learning, large language models, generative AI, and transformers. I am working tirelessly to dig up the most timely and curious tidbits underlying the day’s most popular technologies. I know this field is advancing rapidly and I want to bring you a regular resource to keep you informed and state-of-the-art. The news bites are constantly being added in reverse date order (most recent on top). With the bulletin board you can check back often to see what’s happening in our rapidly accelerating industry. Click HERE to check out previous “AI News Briefs” round-ups. [...] What Anthropic evaluates:\\n\\nWhat the results show:\\n\\n[1/30/2026] How are Stanford students using GenAI? – The use of AI in schoolwork has become as ubiquitous as using Google. Many professors now include a statement on GenAI usage or over-reliance in their syllabi for courses. But a larger question remains: How are students actually using AI? Here is an article appearing in the Stanford Daily campus newspaper.\\n\\n[2/2/2026] Darren Aronofsky debuts AI Revolutionary War series – Filmmaker Darren Aronofsky’s AI venture Primordial Soup released “On This Day… 1776”, a new series recreating the American Revolution using Google DeepMind, with each episode dropping on the 250th anniversary of the event it depicts.\\n\\nKey details: [...] [2/2/2026] Ai2 introduces Theorizer, providing open-source code for automatic theory synthesis from scientific papers – Researchers already automate experiments, but they still read hundreds of papers to extract laws. Theorizer flips that workflow by reading the literature first and writing theories directly. The striking part: it synthesizes explicit scientific laws from up to 100 papers per query. The system treats a theory as compressed knowledge. A law states a repeatable pattern, scope states where that pattern holds, and evidence cites results from real papers. You can think of it as a map from scattered findings to testable claims. You use Theorizer by entering a topic query. The system retrieves open-access papers, extracts structured results, and assembles theories with linked',\n",
       "  'score': 0.9998746},\n",
       " {'title': 'AI News for Feb 2, 2026',\n",
       "  'url': 'https://theautomateddaily.com/e/ai-news-for-feb-2-2026/',\n",
       "  'content': 'Subscribe to edition specific feeds:  \\n- Space news  \\n\\\\ Apple Podcast English Spanish (coming soon) French (coming soon)  \\n\\\\ Spotify English Spanish (coming soon) French (coming soon)  \\n\\\\ RSS English Spanish (coming soon) French (coming soon)  \\n- Top news  \\n\\\\ Apple Podcast English Spanish French  \\n\\\\ Spotify English Spanish French  \\n\\\\ RSS English Spanish French  \\n- Tech news  \\n\\\\ Apple Podcast English Spanish French  \\n\\\\ Spotify English Spanish Spanish  \\n\\\\ RSS English Spanish French  \\n- Hacker news  \\n\\\\ Apple Podcast English Spanish French  \\n\\\\ Spotify English Spanish French  \\n\\\\ RSS English Spanish French  \\n- AI news  \\n\\\\ Apple Podcast English Spanish French  \\n\\\\ Spotify English Spanish French  \\n\\\\ RSS English Spanish French  \\n  \\nVisit our website at [...] \\\\ RSS English Spanish French  \\n  \\nVisit our website at   \\nSend feedback to feedback@theautomateddaily.com  \\nYoutube   \\nLinkedIn   \\nX (Twitter) [...] The Automated Daily\\n\\n# The Automated Daily\\n\\nThe Automated Daily\\n\\n85.2K\\n\\nDownloads\\n\\n6112\\n\\nEpisodes\\n\\nWelcome to ’The Automated Daily’, your ultimate source for a streamlined and insightful daily news experience. Powered by cutting-edge Generative AI technology, we bring you the most crucial headlines of the day, carefully selected and delivered directly to your ears. Our intelligent algorithms scour the news landscape to sift through the noise, ensuring that you receive only the most relevant and significant stories. Join us as we condense the day’s news into a concise and captivating format, keeping you informed and empowered.\\nVisit our website at \\nSend feedback to feedback@theautomateddaily.com\\nLinkedIn - \\nX (Twitter) - \\n\\n## Episodes\\n\\nAI News for Feb 2, 2026\\n\\n2 days ago',\n",
       "  'score': 0.99984527},\n",
       " {'title': 'AI Trends and Regulatory Changes Shaping 2026',\n",
       "  'url': 'https://www.nemko.com/blog/the-outlook-for-ai-in-2026',\n",
       "  'content': 'Organizations may find that in 2026, the next wave of adopting AI (Artificial Intelligence) will not simply be a continuation of 2025 but mark a shift toward more structured, more accountable and more scalable AI operations. AI is moving from experimental workflows and deployment into core business processes, which brings a new set of responsibilities. Across industries, and particularly in domains where reliability, safety and quality are essential, one foresees that the following four major trends will define how organizations navigate the coming year:  \\nFast-Moving AI Regulatory Landscape [...] Autonomous AI Under ControlIn 2026, autonomous AI will drive efficiency while introducing operational risk. Systems can move beyond their intended scope, chain multiple steps unpredictably, amplify errors faster than humans can detect, and interact with each other in ways that create emergent behavior. Managing these risks is becoming a core enterprise competency. For this, the Model Context Protocol (MCP) is emerging as a foundational standard.   \\nResponsible AI ProcurementAI is now entering organizations primarily through procurement rather than internal development [...] In USA, regulatory activity is decentralizing primarily through federal agency mandates. In December 2025, the federal government signaled its intent to limit conflicting state AI laws through Trump’s executive order, aiming to block states from regulating AI companies and strengthen U.S. leadership in AI through low-burden national policy. The regulatory environment becomes dynamic and demanding for industry to keep track of.  \\nScaling AI with Tools & Technology',\n",
       "  'score': 0.9964619},\n",
       " {'title': 'Big moves in an AI data center power stock. What we want from ...',\n",
       "  'url': 'https://www.cnbc.com/2026/02/02/big-moves-in-an-ai-data-center-power-stock-what-we-want-from-tuesdays-earnings.html',\n",
       "  'content': \"CNBC\\nJoin IC\\nJoin Pro\\nJoin IC\\nJoin Pro\\n\\n# Big moves in an AI data center power stock. What we want from Tuesday's earnings\\n\\nthumbnail\\n\\n## More In Morning Meeting\\n\\nCramer says to buy this megacap tech stock ahead of earnings, but not that one\\nTuesday, February 3, 2026: Cramer says this cybersecurity stock will be a great buy despite the pullback\\nMonday, February 2, 2026: Cramer breaks down why this tech giant is ‘such a winner'\\nCNBC logo\\n\\n#### News Tips\\n\\nGot a confidential news tip? We want to hear from you.\\n\\n#### CNBC Newsletters\\n\\nSign up for free newsletters and get more CNBC delivered to your inbox\\n\\nGet this delivered to your inbox, and more info about our products and services.\\n\\n#### Advertise With Us\\n\\nCalifornia Consumer Privacy Act (CCPA) Opt-Out Icon [...] California Consumer Privacy Act (CCPA) Opt-Out Icon\\n\\n© 2026 Versant Media, LLC. All Rights Reserved. A Versant Media Company.\\n\\nData is a real-time snapshot \\\\Data is delayed at least 15 minutes.\\nGlobal Business and Financial News, Stock Quotes, and Market Data\\nand Analysis.\\n\\nData also provided byReuters logo\\n\\nReuters logo\",\n",
       "  'score': 0.96432143},\n",
       " {'title': 'Elon Musk combines his rocket and AI businesses ... - ABC News',\n",
       "  'url': 'https://abcnews.go.com/US/wireStory/musk-billionaire-combines-rocket-ai-businesses-expected-ipo-129796925',\n",
       "  'content': \"“I’ll be surprised if people move from land to low-Earth orbit,” Microsoft’s president, Brad Smith, told The Associated Press last month, when asked about the alternatives to building data centers in the U.S. amid rising community opposition.\\n\\nMusk is already facing stiff competition in artificial intelligence, where he's been scrambling to compete against rivals such as OpenAI, which is also working toward an IPO. Musk's dislike of OpenAI, which he helped to found more than a decade ago, is part of what drove him to start xAI in 2023 and build the ChatGPT alternative he named Grok.\\n\\nMusk has equally ambitious plans for Tesla as he tries to pivot a company with shrinking car sales to focus more on self-driving taxis and humanoid robots, driven by artificial intelligence. [...] His rocket venture, SpaceX, announced on Monday that it had bought xAI in an effort to help the world’s richest man dominate the rocket and artificial intelligence businesses. The deal will combine several of his offerings, including his AI chatbot Grok, his satellite communications company Starlink, and his social media company X.\\n\\nMusk has talked repeatedly about the need to speed development of technology that will allow data centers to operate in space. He believes that will help overcome the problem of huge costs in electricity and other resources in building and running AI systems on Earth.\\n\\nIt's a goal that Musk suggested in his announcement of the deal could become easier to reach with a combined company. [...] “In the long term, space-based AI is obviously the only way to scale,” Musk wrote on SpaceX's website Monday, then added in reference to solar power, “It’s always sunny in space!”\\n\\nMusk said in his announcement he estimates “that within 2 to 3 years, the lowest cost way to generate AI compute will be in space.”\\n\\nSpaceX will be competing in that realm with Google, which is working on a research project called Project Suncatcher that would equip solar-powered satellites with AI computer chips, with a prototype that could launch as soon as next year.\\n\\nBut Musk's prediction of a near future of space-based AI supercomputers is not shared by many other companies building data centers, including Microsoft.\",\n",
       "  'score': 0.1710612}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.invoke(\"Provide me the recent AI news for feb 2nd 2026\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "535d520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the tools in the list\n",
    "### Custom Functions\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# This will be a tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "tools=[arxiv,wikipedia,tavily,add,multiply,divide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bef0136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"groq:llama-3.3-70b-versatile\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf34f66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'gbt4crnj0', 'function': {'arguments': '{\"query\":\"recent AI news\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 744, 'total_tokens': 764, 'completion_time': 0.066749345, 'completion_tokens_details': None, 'prompt_time': 0.073477633, 'prompt_tokens_details': None, 'queue_time': 0.053773095, 'total_time': 0.140226978}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c06d5113ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c24ba-3995-71f0-920b-e1578a5770d7-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'recent AI news'}, 'id': 'gbt4crnj0', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 744, 'output_tokens': 20, 'total_tokens': 764})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "llm_with_tools.invoke([HumanMessage(content=f\"What is the recent AI News\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "218590a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'args': {'query': 'recent AI news'},\n",
       "  'id': 'scbyhggm4',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke([HumanMessage(content=f\"What is the recent AI News\")]).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c12b0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## State Schema\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "class State(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53c3a2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entire Chatbot With LangGraph\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Node definition\n",
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\",\"tool_calling_llm\")\n",
    "\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fce2de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "1706.03762\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (mh6e8yeng)\n",
      " Call ID: mh6e8yeng\n",
      "  Args:\n",
      "    query: 1706.03762\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, base\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  wikipedia (j925cfbgz)\n",
      " Call ID: j925cfbgz\n",
      "  Args:\n",
      "    query: Transformer (machine learning) model\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Transformer (deep learning)\n",
      "Summary: In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the sig\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (thgzrn280)\n",
      " Call ID: thgzrn280\n",
      "  Args:\n",
      "    query: Transformer (machine learning) model\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"9 Transformers – 6.390 - Intro to Machine Learning\", \"url\": \"https://introml.mit.edu/notes/transformers.html\", \"content\": \"## 9.1 Transformers Overview\\n\\nTransformers are powerful neural architectures designed primarily for sequential data, such as text. At their core, transformers are typically auto-regressive, meaning they generate sequences by predicting each token sequentially, conditioned on previously generated tokens. This auto-regressive property ensures that the transformer model inherently captures temporal dependencies, making them especially suited for language modeling tasks like text generation and completion. [...] The transformer architecture processes inputs by applying multiple identical building blocks stacked in layers. Each block performs a transformation that progressively refines the internal representation of the data.\\n\\nSpecifically, each block consists of two primary sub-layers: an attention layer Section 9.4 and a feed-forward network (or multi-layer perceptron) Chapter 6. Attention layers mix information across different positions (or \\\"chunks\\\") in the sequence, allowing the model to effectively capture dependencies regardless of distance. Meanwhile, the feed-forward network significantly enhances the expressiveness of these representations by applying non-linear transformations independently to each position. [...] To address this, transformers incorporate positional embeddings — additional information that encodes the position of each token in the sequence. These embeddings are added to the input token embeddings before any attention layers are applied, effectively injecting ordering information into the model.\\n\\nThere are two main strategies for positional embeddings: (i) learned positional embeddings, where a trainable vector \\\\(p\\\\_i\\\\in\\\\mathbb{R}^d\\\\) is assigned to each position (i.e., token index) \\\\(i=0, 1, 2, ..., n\\\\). These vectors are learned alongside all other model parameters and allow the model to discover how best to encode position for a given task, (ii) fixed positional embeddings, such as sinusoidal positional embedding proposed in the original Transformer paper:\", \"score\": 0.99983907}, {\"title\": \"What is a Transformer Model? - IBM\", \"url\": \"https://www.ibm.com/think/topics/transformer-model\", \"content\": \"1. The model “reads” raw data sequences and converts them into vector embeddings, in which each element in the sequence is represented by its own feature vector(s) that numerically reflect qualities such as semantic meaning.\\n2. The model determines similarities, correlations and other dependencies (or lack thereof) between each vector and each other vector. In most transformer models, the relative importance of one vector to another is determined by computing the dot product between each vector. If the vectors are well aligned, multiplying them together will yield a large value. If they’re not aligned, their dot product will be small or negative. [...] Before training, a transformer model doesn’t yet “know” how to generate optimal vector embeddings and alignment scores. During training, the model makes predictions across millions of examples drawn from its training data, and a loss function quantifies the error of each prediction.  \\n   \\n Through an iterative cycle of making predictions and then updating model weights through backpropagation and gradient descent, the model “learns” to generate vector embeddings, alignment scores and attention weights that lead to accurate outputs.\\n\\n## How do transformer models work?\\n\\nTransformer models such as relational databases generate query, key and value vectorsfor each part of a data sequence,and use them to compute attention weights through a series of matrix multiplications. [...] In autoregressive LLMs, the final layer uses a softmax function to determine the probability that the next word will match each token in its vocabulary “database.” Depending on the specific sampling hyperparameters, the model uses those probabilities to determine the next token of the output sequence.\\n\\n## Transformer models in natural language processing (NLP)\\n\\nTransformer models are most commonly associated with NLP, having originally been developed for machine translation use cases. Most notably, the transformer architecture gave rise to the large language models (LLMs) that catalyzed the advent of generative AI.\", \"score\": 0.9997154}, {\"title\": \"How Transformers Work: A Detailed Exploration of ... - DataCamp\", \"url\": \"https://www.datacamp.com/tutorial/how-transformers-work\", \"content\": \"### What Are Transformer Models?\\n\\nA transformer model is a neural network that learns the context of sequential data and generates new data out of it.\\n\\nTo put it simply:\\n\\n_A transformer is a type of artificial intelligence model that learns to understand and generate human-like text by analyzing patterns in large amounts of text data._\\n\\nTransformers are a current state-of-the-art NLP model and are considered the evolution of the encoder-decoder architecture. However, while the encoder-decoder architecture relies mainly on Recurrent Neural Networks (RNNs) to extract sequential information, Transformers completely lack this recurrency.\\n\\nSo, how do they do it? [...] ### Other Variations\\n\\nThe landscape of foundation models, particularly transformer models, is rapidly expanding. A study identified over 50 significant transformer models, while the Stanford group evaluated 30 of them, acknowledging the field's fast-paced growth. NLP Cloud, an innovative startup part of NVIDIA's Inception program, utilizes around 25 large language models commercially for various sectors like airlines and pharmacies.\\n\\nThere is an increasing trend towards making these models open-source, with platforms like Hugging Face's model hub leading the way. Additionally, numerous Transformer-based models have been developed, each specialized for different NLP tasks, showcasing the model's versatility and efficiency in diverse applications. [...] This pioneering concept was not just a theoretical advancement but also found practical implementation, notably in TensorFlow's Tensor2Tensor package. Furthermore, the Harvard NLP group contributed to this burgeoning field by offering an annotated guide to the paper, supplemented with a PyTorch implementation. You can learn more about how to implement a Transformer from scratch in our separate tutorial.\\n\\nTheir introduction has spurred a significant surge in the field, often referred to as Transformer AI. This revolutionary model laid the groundwork for subsequent breakthroughs in the realm of large language models, including BERT. By 2018, these developments were already being hailed as a watershed moment in NLP.\", \"score\": 0.9995627}, {\"title\": \"Transformers in Machine Learning - GeeksforGeeks\", \"url\": \"https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/\", \"content\": \"geeksforgeeks\\n\\n# Transformers in Machine Learning\\n\\nTransformer is a neural network architecture used for performing machine learning tasks particularly in natural language processing (NLP) and computer vision. In 2017 Vaswani et al. published a paper \\\" Attention is All You Need\\\" in which the transformers architecture was introduced. The article explores the architecture, workings and applications of transformers.\\n\\n## Need For Transformers Model in Machine Learning\\n\\nTransformer architecture uses an attention mechanism to process an entire sentence at once instead of reading words one by one. This is useful because older models work step by step and it helps overcome the challenges seen in models like RNNs and LSTMs.\\n\\nFor example: [...] Transformers apply attention in three different places:\\n\\n1. Encoder Self-Attention\\n\\n2. Decoder Self-Attention (Masked)\\n\\n3. Encoder–Decoder Attention\\n\\nTogether, these three attention types allow the transformer to read the entire input at once and then generate outputs step-by-step with full context.\\n\\n### 7. Softmax Layer for Output Prediction\\n\\nAfter the decoder processes the sequence, it must predict the next token.\\n\\n## Intuition with Example\\n\\nFor instance in the sentence \\\"The cat didn't chase the mouse, because it was not hungry\\\" the word 'it' refers to 'cat'. The self-attention mechanism helps the model correctly associate 'it' with 'cat' ensuring an accurate understanding of sentence structure.\\n\\n## Applications\\n\\nSome of the applications of transformers are:\\n\\n0 Questions [...] \\\\text{FFN}(x) = \\\\max(0, xW\\\\_1 + b\\\\_1)W\\\\_2 + b\\\\_2\\n\\nThis transformation helps refine the encoded representation at each position.\\n\\n### 5. Embeddings\\n\\nTransformers cannot work with raw words as they need numbers. So, each input token (word or subword) is converted into a vector, called an embedding.\\n\\nEmbeddings turn words into meaningful numeric vectors that the transformer can process.\\n\\n### 6. Encoder-Decoder Architecture\\n\\nThe encoder-decoder structure is key to transformer models. The encoder processes the input sequence into a vector, while the decoder converts this vector back into a sequence. Each encoder and decoder layer includes self-attention and feed-forward layers.\\n\\nFor example, a French sentence \\\"Je suis étudiant\\\" is translated into \\\"I am a student\\\" in English.\", \"score\": 0.99950445}, {\"title\": \"Transformer Architecture explained | by Amanatullah - Medium\", \"url\": \"https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c\", \"content\": \"Transformer models are one of the most exciting new developments in machine learning. They were introduced in the paper Attention is All You Need. Transformers can be used to write stories, essays, poems, answer questions, translate between languages, chat with humans, and they can even pass exams that are hard for humans! But what are they? You’ll be happy to know that the architecture of transformer models is not that complex, it simply is a concatenation of some very useful components, each of which has its own function. In this chapter, you will learn all of these components. [...] I have to be honest with you, the first time I found out that transformers build text one word at a time, I couldn’t believe it. First of all, this is not how humans form sentences and thoughts. We first form a basic thought, and then start refining it and adding words to it. This is also not how ML models do other things. For example, images are not built this way. Most neural network based graphical models form a rough version of the image, and slowly refine it or add detail until it is perfect. So why would a transformer model build text word by word? One answer is, because that works really well. A more satisfying one is that because transformers are so incredibly good at keeping track of the context, that the next word they pick is exactly what it needs to keep going with an idea. [...] Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n# Transformer Architecture explained\\n\\nAmanatullah\\n\\n10 min read\\n\\n·\\n\\nSep 1, 2023\\n\\n--\\n\\nTransformers are a new development in machine learning that have been making a lot of noise lately. They are incredibly good at keeping track of context, and this is why the text that they write makes sense. In this chapter, we will go over their architecture and how they work.\", \"score\": 0.99937373}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The transformer model is a type of artificial neural network architecture that is primarily used for natural language processing (NLP) tasks. It was introduced in the paper \"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. The transformer model is known for its ability to handle long-range dependencies in sequential data, such as text, and has been widely used for tasks such as machine translation, text generation, and language modeling. The transformer model uses a self-attention mechanism to weigh the importance of different words in a sentence, allowing it to capture complex relationships between words. The model consists of an encoder and a decoder, with the encoder taking in a sequence of words and the decoder generating a sequence of words based on the output of the encoder. The transformer model has been shown to be highly effective for many NLP tasks and has been widely adopted in the field.\n"
     ]
    }
   ],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=\"1706.03762\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3846a31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Provide me the top 10 recent AI news for February 2nd 2026,add 69 and 69 and then multipy it by 62\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (mtt9a9hj0)\n",
      " Call ID: mtt9a9hj0\n",
      "  Args:\n",
      "    query: top 10 recent AI news February 2nd 2026\n",
      "  add (b2c44nhf1)\n",
      " Call ID: b2c44nhf1\n",
      "  Args:\n",
      "    a: 69\n",
      "    b: 69\n",
      "  multiply (1063ksna2)\n",
      " Call ID: 1063ksna2\n",
      "  Args:\n",
      "    a: 138\n",
      "    b: 62\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"AI News Briefs BULLETIN BOARD for February 2026\", \"url\": \"https://radicaldatascience.wordpress.com/2026/02/02/ai-news-briefs-bulletin-board-for-february-2026/\", \"content\": \"Welcome to the AI News Briefs Bulletin Board, a timely new channel bringing you the latest industry insights and perspectives surrounding the field of AI including deep learning, large language models, generative AI, and transformers. I am working tirelessly to dig up the most timely and curious tidbits underlying the day’s most popular technologies. I know this field is advancing rapidly and I want to bring you a regular resource to keep you informed and state-of-the-art. The news bites are constantly being added in reverse date order (most recent on top). With the bulletin board you can check back often to see what’s happening in our rapidly accelerating industry. Click HERE to check out previous “AI News Briefs” round-ups. [...] structured results, and assembles theories with linked evidence. [...] What Anthropic evaluates:\\n\\nWhat the results show:\\n\\n[1/30/2026] How are Stanford students using GenAI? – The use of AI in schoolwork has become as ubiquitous as using Google. Many professors now include a statement on GenAI usage or over-reliance in their syllabi for courses. But a larger question remains: How are students actually using AI? Here is an article appearing in the Stanford Daily campus newspaper.\\n\\n[2/2/2026] Darren Aronofsky debuts AI Revolutionary War series – Filmmaker Darren Aronofsky’s AI venture Primordial Soup released “On This Day… 1776”, a new series recreating the American Revolution using Google DeepMind, with each episode dropping on the 250th anniversary of the event it depicts.\\n\\nKey details:\", \"score\": 0.9995122}, {\"title\": \"AI News Roundup - February 02, 2026 - AI Futures Forum\", \"url\": \"https://aiforum.org.uk/ai-news-roundup-february-02-2026/\", \"content\": \"### The Sequence Radar #799: The Week AI Leveled Up: From Chatbots to World Builders\\n\\nSource: TheSequence | Published: 2026-02-01\\n\\nThis week marked a significant leap in AI capabilities, moving beyond simple chatbots to advanced “world building” technologies. The surge is driven by two major releases from Chinese developers alongside massive announcements from industry giants Google and OpenAI.\\n\\n### AI ‘slop’ is transforming social media – and a backlash is brewing\\n\\nSource: BBC News | Published: 2026-02-02 [...] ### Moltbook: The Good, The Bad, and the FUTURE\\n\\nSource: David Shapiro’s Substack | Published: 2026-02-01\\n\\nMoltbook introduces a new AI framework focused on swarm intelligence, allowing multiple AI agents to collaborate on complex tasks simultaneously. While promising significant future advancements in automation (“The Good” and “The Future”), the system likely presents new challenges regarding control and coordination (“The Bad”).\\n\\n### Why AI is slowing down in 2026\\n\\nSource: David Shapiro’s Substack | Published: 2026-01-29 [...] Begin typing your search above and press return to search. Press Esc to cancel.\\n\\n# AI Futures Forum\\n\\n## Primary Navigation\\n\\n# AI News Roundup – February 02, 2026\\n\\n### The AI Hype Index: Grok makes porn, and Claude Code nails your job\\n\\nSource: Artificial intelligence – MIT Technology Review | Published: 2026-01-29\\n\\nThe AI landscape currently presents a stark dichotomy, with Elon Musk’s Grok criticised for generating explicit content while Anthropic’s Claude Code demonstrates impressive capabilities in automating complex professional tasks. This unpredictable mix of incompetence and hyper-competence is fuelling widespread anxiety, particularly among Gen Z, about the future impact of artificial intelligence.\\n\\n### Anthropic Moves Into Legal Tech\", \"score\": 0.9992206}, {\"title\": \"The Latest AI News and AI Breakthroughs\", \"url\": \"https://www.crescendo.ai/news/latest-ai-news-and-updates\", \"content\": \"Date: February 2, 2026\\n\\nSummary: In a landmark consolidation of his business empire, Elon Musk has announced a merger between SpaceX and his artificial intelligence venture, xAI. The strategic move aims to deeply embed xAI’s \\\"Grok\\\" models into SpaceX’s operations to accelerate the development of fully autonomous spacecraft and robotic Mars colonies. By combining world-leading aerospace engineering with advanced generative AI, the new entity intends to automate complex mission trajectories and real-time decision-making for deep-space missions. Analysts suggest the merger will create a \\\"technological powerhouse\\\" capable of outpacing traditional competitors through the rapid application of AI-driven simulation and design.\\n\\nSource: The Guardian ↗ [...] and scale engineering and go-to-market teams as it aims for “perfect search” built for AI. [...] Date: November 26, 2025\", \"score\": 0.9984988}, {\"title\": \"2/2/2026 | Daily AI News from GAI Insights - YouTube\", \"url\": \"https://www.youtube.com/watch?v=AdfUzQgoLpw\", \"content\": \"Welcome to today's edition of Daily AI News by GAI Insights! Every weekday morning at 7:30 am ET, our expert analysts curate and discuss the\", \"score\": 0.99746895}, {\"title\": \"Top Technology & AI News Updates from Week 2 2026 (4th Jan\", \"url\": \"https://www.linkedin.com/pulse/top-technology-ai-news-updates-from-week-2-2026-4th-jan-shankar-yag8c\", \"content\": \"1. 🇮🇳 National Startup Day 2026: A Decade of Startup India & The Road Ahead As India marks National Startup Day on 16…\\n\\n### 🚀 On Thiruvalluvar Day, learning how to build startups from timeless wisdom\\n\\nWe often look to Silicon Valley for the latest frameworks on innovation, but some of the best advice for founders was…\\n\\n### GenAI Goes beyond Agentic: What the First Week of 2026 Reveals About the Global AI Shift\\n\\nFrom voice-native AI in the US to sovereign agents in the Middle East and AI IPOs in China The first week of 2026 made…\\n\\n### 🚀 THub Launches Next-Gen Features for Enterprise AI\\n\\nAgent Studio | Context Engineering | MCP Integrations | Agent-to-Agent Protocol | Eval & Observability THub…\\n\\n### THub Architecture, Explained: Orchestrating Agentic AI for the Enterprise [...] ### The Growing Gap Between Deep Tech Demand and Talent Supply\\n\\n## Explore content categories [...] Article content\\n\\nOn January 8, 2026, Microsoft announced a new suite of agentic AI solutions for retail, designed to deliver intelligent automation across the entire retail value chain—from merchandising and marketing to store operations and fulfillment.\\n\\nRather than point AI tools, Microsoft is positioning agentic AI as a connected operating layer that unifies fragmented retail workflows and augments human expertise with context-aware, autonomous decision support.\\n\\n### 🔑 What’s New\\n\\nMicrosoft’s agentic AI capabilities are built to help retailers:\\n\\n Move faster with automated, coordinated execution\\n Deliver highly personalized shopping experiences\\n Improve operational resilience and efficiency\\n Turn AI insights into real-time actions, not dashboards\", \"score\": 0.996852}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "138\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "8556\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are the top 10 recent AI news for February 2nd, 2026:\n",
      "\n",
      "1. AI News Briefs BULLETIN BOARD for February 2026 (radicaldatascience.wordpress.com)\n",
      "2. AI News Roundup - February 02, 2026 - AI Futures Forum (aiforum.org.uk)\n",
      "3. The Latest AI News and AI Breakthroughs (crescendo.ai)\n",
      "4. 2/2/2026 | Daily AI News from GAI Insights - YouTube (youtube.com)\n",
      "5. Top Technology & AI News Updates from Week 2 2026 (4th Jan (linkedin.com)\n",
      "6. The AI Hype Index: Grok makes porn, and Claude Code nails your job (mittechnologyreview.com)\n",
      "7. Anthropic Moves Into Legal Tech (mittechnologyreview.com)\n",
      "8. GenAI Goes beyond Agentic: What the First Week of 2026 Reveals About the Global AI Shift (linkedin.com)\n",
      "9. THub Launches Next-Gen Features for Enterprise AI (linkedin.com)\n",
      "10. The Growing Gap Between Deep Tech Demand and Talent Supply (linkedin.com)\n",
      "\n",
      "The result of adding 69 and 69 is 138. \n",
      "The result of multiplying 138 by 62 is 8556.\n"
     ]
    }
   ],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=\"Provide me the top 10 recent AI news for February 2nd 2026,add 69 and 69 and then multipy it by 62\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a75c5ac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'tool call validation failed: attempted to call tool \\'wikipedia{\"query\": \"Machine learning\"}\\' which was not in request.tools', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=wikipedia{\"query\": \"Machine learning\"}></function>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m messages=\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is machine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m      3\u001b[39m     m.pretty_print()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtool_calling_llm\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtool_calling_llm\u001b[39m(state:State):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[43mllm_with_tools\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/langchain_groq/chat_models.py:593\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    589\u001b[39m params = {\n\u001b[32m    590\u001b[39m     **params,\n\u001b[32m    591\u001b[39m     **kwargs,\n\u001b[32m    592\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG_learnings/.venv/lib/python3.12/site-packages/groq/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'tool call validation failed: attempted to call tool \\'wikipedia{\"query\": \"Machine learning\"}\\' which was not in request.tools', 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=wikipedia{\"query\": \"Machine learning\"}></function>'}}",
      "During task with name 'tool_calling_llm' and id 'f0a5835b-1f60-3afa-5c77-7eff12a58fdc'"
     ]
    }
   ],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=\"What is machine learning\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e072e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_learnings (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
