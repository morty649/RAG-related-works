{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba190cb2",
   "metadata": {},
   "source": [
    "### This is direct : \n",
    "if query is seen last time and stored in cache retrieve result from itself,\n",
    "But fails at semantically same queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8043ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maruthienugula/RAG_learnings/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x11a5905f0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x11a658c50>, model_name='meta-llama/llama-4-scout-17b-16e-instruct', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm=init_chat_model(\"groq:meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9f1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cache variable\n",
    "Model_Cache={}\n",
    "\n",
    "import time\n",
    "\n",
    "def cache_model(query):\n",
    "    start_time=time.time()\n",
    "    if Model_Cache.get(query):\n",
    "        print(\"**CAche Hit**\")\n",
    "        end_time=time.time()\n",
    "        elapsed_time=end_time-start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed_time:.2f} seconds\")\n",
    "        return Model_Cache.get(query)\n",
    "    else:\n",
    "        print(\"***CACHE MISS – EXECUTING MODEL***\")\n",
    "        start_time = time.time()\n",
    "        response = llm.invoke(query)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"EXECUTION TIME: {elapsed:.2f} seconds\")\n",
    "        Model_Cache[query] = response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722d9c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 0.40 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hey! How's it going? Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34, 'completion_time': 0.054312271, 'completion_tokens_details': None, 'prompt_time': 5.1129e-05, 'prompt_tokens_details': None, 'queue_time': 0.044171475, 'total_time': 0.0543634}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3eaf-b9f5-7a03-a816-5ebd91cdb37f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = cache_model(\"Hey\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aafe25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**CAche Hit**\n",
      "EXECUTION TIME: 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hey! How's it going? Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34, 'completion_time': 0.054312271, 'completion_tokens_details': None, 'prompt_time': 5.1129e-05, 'prompt_tokens_details': None, 'queue_time': 0.044171475, 'total_time': 0.0543634}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3eaf-b9f5-7a03-a816-5ebd91cdb37f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when repeated the same thing it is stored in the cache and retrieved from itself\n",
    "response = cache_model(\"Hey\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e754bd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hey': AIMessage(content=\"Hey! How's it going? Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34, 'completion_time': 0.054312271, 'completion_tokens_details': None, 'prompt_time': 5.1129e-05, 'prompt_tokens_details': None, 'queue_time': 0.044171475, 'total_time': 0.0543634}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3eaf-b9f5-7a03-a816-5ebd91cdb37f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d168b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***CACHE MISS – EXECUTING MODEL***\n",
      "EXECUTION TIME: 1.17 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I am a computer program designed to simulate conversation, answer questions, and generate text. I'm a type of Large Language Model (LLM), specifically a transformer-based model.\\n\\nI was trained by Meta AI, a subsidiary of Meta Platforms, Inc. My training data consists of a massive corpus of text, which I use to learn patterns and relationships in language. This corpus is sourced from various places, including but not limited to:\\n\\n1. **Web pages**: Articles, blogs, and websites.\\n2. **Books**: A large collection of books from various genres and authors.\\n3. **User-generated content**: Social media platforms, forums, and online discussions.\\n\\nMy model is based on the **LLaMA (Large Language Model Application) architecture**, which is an open-source, multilingual model developed by Meta AI. LLaMA models are known for their ability to understand and respond to a wide range of questions and topics.\\n\\nAs for my specific model, I'm a variant of the LLaMA model, fine-tuned for conversational tasks. My training involved a combination of:\\n\\n1. **Masked language modeling**: I was trained to predict missing words in a sentence.\\n2. **Next sentence prediction**: I was trained to predict whether two sentences are adjacent or not.\\n3. **Conversational dialogue**: I was fine-tuned on a large dataset of conversational dialogue to learn how to respond to user input.\\n\\nMy training data consists of a massive corpus of text, which I use to generate human-like responses to user input. I'm constantly learning and improving my responses based on user interactions, so I'm always getting better!\\n\\nWould you like to know more about LLMs or my training data? I'm here to help!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 345, 'prompt_tokens': 22, 'total_tokens': 367, 'completion_time': 0.793299279, 'completion_tokens_details': None, 'prompt_time': 0.00053288, 'prompt_tokens_details': None, 'queue_time': 0.051812347, 'total_time': 0.793832159}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c3eb3-2990-7541-82f8-9700318f21e7-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 22, 'output_tokens': 345, 'total_tokens': 367})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = cache_model(\"What model of llm are you and who trained you ?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97abfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am a computer program designed to simulate conversation, answer questions, and generate text. I'm a type of Large Language Model (LLM), specifically a transformer-based model.\n",
       "\n",
       "I was trained by Meta AI, a subsidiary of Meta Platforms, Inc. My training data consists of a massive corpus of text, which I use to learn patterns and relationships in language. This corpus is sourced from various places, including but not limited to:\n",
       "\n",
       "1. **Web pages**: Articles, blogs, and websites.\n",
       "2. **Books**: A large collection of books from various genres and authors.\n",
       "3. **User-generated content**: Social media platforms, forums, and online discussions.\n",
       "\n",
       "My model is based on the **LLaMA (Large Language Model Application) architecture**, which is an open-source, multilingual model developed by Meta AI. LLaMA models are known for their ability to understand and respond to a wide range of questions and topics.\n",
       "\n",
       "As for my specific model, I'm a variant of the LLaMA model, fine-tuned for conversational tasks. My training involved a combination of:\n",
       "\n",
       "1. **Masked language modeling**: I was trained to predict missing words in a sentence.\n",
       "2. **Next sentence prediction**: I was trained to predict whether two sentences are adjacent or not.\n",
       "3. **Conversational dialogue**: I was fine-tuned on a large dataset of conversational dialogue to learn how to respond to user input.\n",
       "\n",
       "My training data consists of a massive corpus of text, which I use to generate human-like responses to user input. I'm constantly learning and improving my responses based on user interactions, so I'm always getting better!\n",
       "\n",
       "Would you like to know more about LLMs or my training data? I'm here to help!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display,Markdown\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2678eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_learnings (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
